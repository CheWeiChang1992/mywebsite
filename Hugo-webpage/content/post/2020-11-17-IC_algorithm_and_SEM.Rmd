---
title: "Causal structure inference: IC algorithm and SEM"
author: "Che-Wei Chang"
date: "2020-12-07"
output: html_document

bibliography: bib/ref.bib
biblio-style: natbib
---
## Why we study causal structure of traits?
In a biological system, one trait may contribute to another and result in a complex pathway network. If causal relationships exist, we can expect that unselected traits will change when we select specific traits. We can use this feature to indirectly improve a target trait that is difficult or expensive to measure or has low heritability. Also, this feature may cause an undesirable change in traits that we want to maintain. Therefore, understanding causal structures can be helpful for us to define our breeding strategies.  
  
  
I would briefly introduce a framework of causal structure analysis consisting of structural equation models and inductive causation algorithm based on @valente2010searching and @inoue2020application.  

**To perform causal structure analysis by using this framework, please see the book chapter of @valente2013mixed for detailed tutorials.**

## Structural equation models (SEM)
Here, we first introduce the structural equation models [SEM; @Wright1921CorrelationAndCausation; @haavelmo1943statistical], which allows the inference of causal coefficients. To fit SEMs, a known causal relationship is required. However, we usually have no prior knowledge or very limited knowledge of causal structures. In this situation, we need to recover a causal network by using Bayesian network (BN) learning algorithms. In the next section, we will look at the inductive causation (IC) algorithm, which has been used in several animal studies.    

A SEM with a causal structure for subject $i$ (assuming $i = 1\ ...\ n$) can be written as:  

$y_i=\Lambda y_i+X_i\beta +Zu_i+e_i$   

where $\Lambda$ is a $(t\times t)$ matrix of structural coefficients with zeroes in the diagonal and with structural coefficients in the off-diagonal. $t$ is the number of traits. $y_i$ is a multi-trait $(t\times 1) vector$ of subject $i$ th; $u$ and $e$ represent random genetic effects and residuals (phenotypic effects are included in residuals) respectively  
  
Then, we can further have:  
$(I-\Lambda)y_i=X_i\beta+Zu_i+e_i$  
  
    
Finally, we get    
$y_i = (I-\Lambda)^{-1}X_i\beta + (I-\Lambda)^{-1}Zu_i + (I-\Lambda)^{-1}e_i$   

with a joint distribution:     
  
$\begin{bmatrix} u_i \\ e_i \end{bmatrix} \sim N \begin{Bmatrix}\begin{bmatrix}0\\0\end{bmatrix} ,\begin{bmatrix}(I-\Lambda)^{-1}G[(I-\Lambda)^{-1}]^T & 0\\ 0 & (I-\Lambda)^{-1}R[(I-\Lambda)^{-1}]^T\end{bmatrix} \end{Bmatrix}= N \begin{Bmatrix}\begin{bmatrix}0\\0\end{bmatrix} ,\begin{bmatrix}G' & 0\\ 0 & R'\end{bmatrix}\end{Bmatrix}$  
  
We extend equation to $n$ subjects:  
  
$y = (I-\Lambda\otimes I_n)^{-1}X\beta + (I-\Lambda\otimes I_n)^{-1}Zu + (I-\Lambda\otimes I_n)^{-1}e$   
  
with $\begin{bmatrix} u \\ e \end{bmatrix} \sim N \begin{Bmatrix}\begin{bmatrix}0\\0\end{bmatrix} ,\begin{bmatrix}G'\otimes A & 0\\ 0 & R'\otimes I_n\end{bmatrix}\end{Bmatrix}$ where $A$ is a genetic relationship matrix.

@gianola2004quantitative suggested a Bayesian framework for the SEM inference. Please look at their paper [@gianola2004quantitative] if you are interested in details.
   


## Inductive causation (IC) algorithm
Conventionally, the inference of causal coefficients requires prior knowledge of phenotypes under studying. However, if causal relationships are unclear, it is not possible to perform inference. Additionaly, our inference can be limited to the known causal relationship and ignore other possibilities even if we have prior knowledge.   
  
To solve the mentioned problem, @valente2010searching implemented the IC algorithm [@verma1990equivalence; @pearl2000models] as a data-driven approach to indentify causal relationships. In this way, we can investigate the causal structure even if we have zero or limited prior knowledge!  

### (1) Assumption of IC algorithm
The IC algorithm assumes that no hidden variables affect more than one of the variable, which are included in the searching process, that is, **causal sufficiency assumption**. We will have a misleading result if our data violates this assumption.   
  
For instance, **if there are observed variables `a` and `b` sharing a common causeal variable `c`, but `c` is unavailable in our data, this will result in a misleading edge between `a` and `b` in the oriented graph because the IC algorithm cannot identify the correct causal structure (correlation attributes to a common hidden cause) by conditioning the correlation of `a` and `b` on `c`**.  


### (2) confounding effect of genetic relationship
As genetic effects also contribute to traits, @valente2010searching considered random genetic effects in a multiple-trait model to exclude the genetic effects on causal relationships before performing the IC algorithm.  
  
A mixed model of traits can be written as what mentioned above:  
  
$y_i = (I-\Lambda)^{-1}X_i\beta + (I-\Lambda)^{-1}Zu_i + (I-\Lambda)^{-1}e_i$   
  

  
with $\begin{bmatrix} u_i \\ e_i \end{bmatrix} \sim N \begin{Bmatrix}\begin{bmatrix}0\\0\end{bmatrix} ,\begin{bmatrix}G' & 0\\ 0 & R'\end{bmatrix}\end{Bmatrix}$
   

Thus,  
$Var(y_i) = G' + R'$  

By conditioning on genetic effects, we can have:  
$Var(y_i|u_i) = R'$  

@valente2010searching used a Bayesian method to infer the posterior distribution of $R'$ and used that posterior distribution as the input of the IC algorithm.  

### (3) How does IC algorithm work?
The IC algorithm is aiming to generate an oriented network graph that connects traits by directed edges. Its procedure can roughly divided into three steps.  

#### Step 1: identify adjacent variables in an undirected graph
Let's assume we have three traits, $T = \{t1, t2, t3\}$, and their true relationship is $t1\gets t2\to t3$. Thus, we should find three traits correlate with each other (correlation coeffeicient $\rho_{t1,t2} \ne 0; \rho_{t1,t3}\ne 0; \rho_{t2,t3}\ne 0$) but, theoretically, the partial correlation $\rho_{t1,t3|t2} = 0$ while $\rho_{t1,t2|t3}\ne0$ and $\rho_{t2,t3|t1}\ne0$.  
  
With this feature, we can infer an undirected graph that $t1$ and $t3$ are not neighbor ($t1-t2-t3$).  

In a real analysis, we will have more complex structure, but the general idea is the same. If we can find two variables, $a$ and $b$, have non-zero partial correlation when conditioning on any remaining variables ($\rho_{a,b|S_{ab}}\ne 0$ where $S_{ab}$ is a subset of variables that $S_{ab}\notin \{a,b\}$), then we can declare the variable $a$ and $b$ are adjacent in a network graph because no observed variable can block the correlation of $a$ and $b$.  
  
In the approach of @valente2010searching, the posterior distribution of $\rho_{a,b|S_{ab}}$ is used to declare edges between traits. If the 95% highest posterior density (HPD) intervals contain 0, $\rho_{a,b|S_{ab}}$ is considered to be null. In practical use, different HPD thresholds usually are tested [see the review of @inoue2020application].   

#### Step 2: orient undirected edges based on $\rho_{a,b|S_{ab}}$
In the example we discussed here, it is easy to conclude the causal structure can be $t1\to t2\to t3$, $t1\gets t2\gets t3$, or $t1\gets t2\to t3$ as conditioning on $t2$ can block the correlation of $t1$ and $t3$ resulting $\rho_{t1,t3|t2}=0$.  

Unfortunately, we cannot do any further inference of orientation in our case since we have no additional information to tell which causal relationship ($t1\to t2\to t3$, $t1\gets t2\gets t3$, or $t1\gets t2\to t3$) is correct.

Considering a case described by @valente2010searching, if we find a structure $a -c-b$ that $a$ and $b$ share a common adjacent variable $c$. If we cannot find any variable set $S_{ab}$ including $c$ that can result in $\rho_{a,b|S_{ab}}= 0$, we then infer $a \to c \gets b$. Otherwise, if $S_{ab}$ exists, we infer $a \gets c \to b$ and continue to the next step.     
  
#### Step 3: orient undirected edges as many as possible
If there are still undirected edges, we then need to orient them by the following rules:  

* Do not create a new collider (e.g. if we have $t1\to t2- t3$, we should orient edge as $t1\to t2\to t3$ rather than $t1\to t2\gets t3$ because the latter creates a collider.)  
* Do not create a cycle according to the assumption of directed acyclic graphs (DAG).  
* Use your prior knowledge regarding traits.  










## Reference